# Quick Start - RAG Enhancement & Model Testing

## üöÄ Get Started in 3 Steps

### Step 1: Ensure Ollama is Running

```powershell
# Start Ollama (if not already running)
ollama serve

# Pull the REQUIRED model (tested and optimized)
ollama pull qwen2.5-coder:3b
```

**Note**: The application is configured to use **qwen2.5-coder:3b exclusively**. This model was selected after comprehensive testing and provides the best balance of accuracy (85%), quality (70.3%), and speed (40.9 tokens/sec) for LPC development.

### Step 2: Run Model Comparison

```powershell
cd e:\Work\lpc-development-assistant

# Run the comparison (takes 2-10 minutes)
cargo run --bin model_comparison

# Output saved to: model_comparison_results.json
```

**Expected Output**:
```
LPC Development Assistant - Model Comparison Tool
===================================================

Checking Ollama connection...
Available models: ["deepseek-coder:latest", "codellama:latest", "llama3:latest", "mistral:latest"]

Loading MUD reference index...
  Indexed 25427 files

Initializing RAG validator...
  Loaded 166 known efuns

Testing models: ["deepseek-coder:latest", ...]

=== COMPARISON RESULTS ===

Best Accuracy:  deepseek-coder:latest
Best Speed:     mistral:latest
Best Quality:   deepseek-coder:latest

RECOMMENDED MODEL: deepseek-coder:latest

Based on testing: deepseek-coder:latest achieved highest accuracy (87.3%), 
mistral:latest was fastest (avg 1234ms), deepseek-coder:latest had best 
quality score (89.1%). Recommended model balances all factors.
```

### Step 3: Try Enhanced Query Tool

```powershell
# Start interactive query session
cargo run --bin enhanced_query

# Select a model (usually #1 - deepseek-coder)
# Then ask LPC questions!
```

**Example Session**:
```
LPC Development Assistant - Enhanced Query CLI
================================================

Initializing...
  ‚úì Indexed 25427 files
  ‚úì Loaded 166 known efuns

Available models:
  1. deepseek-coder:latest
  2. codellama:latest
  3. llama3:latest
  4. mistral:latest

Select model (1-4, default 1): 1

Using model: deepseek-coder:latest

Type your LPC questions (or 'quit' to exit)

/> How do I use call_other()?

üîç Searching knowledge base...

üìö Source Validation:
  Confidence Score: 87.3%
  Validation Score: 91.2%
  Sources Consulted: 15
  Efuns Found: call_other, apply, call_out

üìÑ Top Sources:
  1. [‚úì] interpret.c (line 1234) - 92%
  2. [‚úì] efun_defs.c (line 567) - 88%
  3. [‚äï] object.c (line 890) - 85%
  ...

ü§ñ Generating response with deepseek-coder:latest model...

üìù Answer:
======================================================================
[Detailed explanation with code examples]
======================================================================

Answer based on 15 sources from the MUD reference corpus

/> quit
Goodbye!
```

---

## üìä What You Get

### Model Comparison Results
- JSON file with complete metrics
- Best model recommendation
- Detailed accuracy, quality, and speed scores
- Per-query breakdown

### Enhanced Query Benefits
- Real-time source validation
- Confidence scores for every answer
- Citations showing which files were consulted
- Efun detection and listing
- Code examples from actual MUD implementations

---

## üéØ Test Queries to Try

Copy these into the enhanced query tool:

```
1. How do I implement a combat system in LPC?

2. Show me the correct syntax for query_* functions

3. What's the difference between call_other() and call_out()?

4. Generate a basic room inherit for the std/room.c

5. How does the LPC object inheritance system work?

6. How do I use mappings in LPC?

7. What are simul_efuns and how do I create them?

8. Show me how to implement a save/restore system

9. How do I use call_out() for timed events?

10. What's the best way to implement object inheritance?
```

---

## üîç Understanding the Output

### Validation Status Icons
- ‚úì **Verified**: Found in 3+ sources (highest confidence)
- ‚äï **Cross-Referenced**: Found in 2 sources (good confidence)
- ‚óã **Single Source**: Only one source (use caution)
- ? **Uncertain**: Conflicting information (needs review)

### Confidence Scores
- **90-100%**: Excellent - information verified across many sources
- **70-89%**: Good - solid information with cross-references
- **50-69%**: Fair - limited sources, verify before use
- **Below 50%**: Low - may need more specific query

---

## üêõ Troubleshooting

### "Failed to create Ollama client"
```powershell
# Ensure Ollama is running
ollama serve
```

### "No models found"
```powershell
# List available models
ollama list

# Pull at least one model
ollama pull deepseek-coder
```

### Slow first run
- Index building takes 30-60 seconds first time
- This is normal - subsequent queries are fast
- Future enhancement: persistent caching

### Low confidence scores
- Try more specific questions
- Check efun spelling
- Add more context to your query

---

## üìñ Learn More

- Full documentation: [`docs/RAG_ENHANCEMENT_GUIDE.md`](RAG_ENHANCEMENT_GUIDE.md)
- Implementation details: [`docs/IMPLEMENTATION_SUMMARY.md`](IMPLEMENTATION_SUMMARY.md)

---

## Next Steps

1. ‚úÖ Run model comparison to find your best model
2. ‚úÖ Test enhanced query with your actual LPC questions
3. ‚úÖ Review the results JSON for detailed metrics
4. ‚úÖ Integrate the recommended model into your workflow

**Ready to build amazing MUDs!** üéÆ
